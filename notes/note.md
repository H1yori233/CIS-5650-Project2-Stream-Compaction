# Note



## 基本概念与性能指标

### 指令与操作
- **指令(Instruction)**：告诉机器执行的具体动作（如：MUL、ADD）
- **操作(Operation)**：计算单元执行的具体运算（如：FLT32、INT操作）
- **关系**：对于warp大小为32的情况，1条指令 = 32个操作

### 时钟与性能
- **时钟周期(Clock Cycle)**：指令执行的基本时间单位
- **时钟速度(Clock Speed)**：每秒执行的周期数（如：1GHz = 每秒10亿周期）

### 关键性能指标
- **延迟(Latency)**
  - 从发出指令到处理完数据的延迟时间
  - 例如：全局内存读取需要200个周期

- **吞吐量(Throughput)**
  - 单位时间内处理的指令/数据量
  - 衡量标准：每周期指令数(IPC)等
- **带宽(Bandwidth)**
  - 数据传输速率
  - 例如：RTX 4080的显存带宽为736 GB/s



## CPU架构与设计

### 性能与应用特征
- **计算性能度量**
- **FLOPS (FLoating-point OPerations per Second)**
  - 每秒浮点运算次数
  - **GFLOPS**: 10^9 FLOPS (十亿次每秒)
  - **TFLOPS**: 10^12 FLOPS (万亿次每秒)
- **性能发展趋势**
- **CPU单核性能**
  - 2003年后增长显著放缓
  - 主要受限于功耗和散热问题
- **GPU性能优势**
  - 每瓦特(FLOP/watt)性能更高
  - 每平方毫米(FLOP/mm²)性能更高
  - 每美元(FLOP/$)性能更高
- **实际应用案例**
- **NVIDIA DRIVE PX 2 自动驾驶计算平台**
  - 2个Tegra SoC: 2.5 TFLOPS
  - 2个Pascal GPU: 5 TFLOPS
  - 总计算力: 7.5 TFLOPS
- **桌面应用程序特点**
- 线程数较少(Lightly threaded)
- 大量分支指令(Lots of branches)
- 大量内存访问(Lots of memory accesses)
- 指令分布统计:
  * 条件分支(Conditional branches): 12.5-13.6%
  * 内存访问(Memory accesses): 45.7%
  * 向量指令(Vector instructions): 0.2-1.1%



### 核心架构与执行优化

- **CPU核心架构**
- **基本流水线**
  - 五个阶段：取指(Fetch) → 解码(Decode) → 执行(Execute) → 访存(Memory) → 写回(Writeback)
  - 通过流水线技术提高指令执行效率
  - 核心组件包括寄存器文件(Register File)
- **分支预测**
- **现代预测器**
  - 准确率超过90%
  - 提升性能和能效
- **潜在问题**
  - 增加芯片面积
  - 可能增加取指阶段延迟
  - 安全隐患(如Spectre/Meltdown漏洞)
- **内存层次结构**
- **基本特性**：容量越大，访问速度越慢
- **各级存储性能对比**：
  | 存储类型 | 延迟 | 带宽 | 容量 |
  |----------|------|------|------|
  | SRAM(L1-L3) | 1-2ns | 200-3000GB/s | 120MB |
  | DRAM(内存) | 70ns | 200GB/s | 1-20GB |
  | Flash/SSD | 70-90μs | 200-5000MB/s | 100-1000GB |
  | HDD(硬盘) | 10ms | 1-150MB/s | 500-3000GB |
- **缓存系统**
- **基本原理**：保持频繁访问的数据在近端
- **局部性原理**
  - 时间局部性：最近使用的数据可能很快再次使用
  - 空间局部性：下一个使用的数据可能在之前数据附近
- **缓存层次**
  - 硬件管理
    * L1：指令缓存和数据缓存
    * L2：统一缓存
    * L3：统一缓存
  - 软件管理
    * 主内存
    * 磁盘
- **性能优化技术**
- **IPC优化**
  - 超标量技术：增加流水线宽度
- **指令调度**
  - 处理数据依赖
    * RAW(Read-After-Write)：读后写依赖
    * WAW(Write-After-Write)：写后写依赖
  - 寄存器重命名：解决假依赖
- **乱序执行**
  - 重排指令以最大化吞吐量
  - 关键组件：
    * ROB(Reorder Buffer)：跟踪指令状态
    * PRF(Physical Register File)：物理寄存器文件
    * Issue Queue/Scheduler：选择下一条执行指令



### 并行化技术

- **CPU并行化技术**
- **三种并行级别**
  1. **指令级并行(ILP)**
     - 超标量处理器：同时执行多条指令
     - 乱序执行：打破指令顺序限制
  2. **数据级并行(DLP)**
     - 向量操作：同时处理多个数据元素（Vector）
  3. **线程级并行(TLP)**
     - 同时多线程(SMT)
     - 多核处理器
- **数据级并行**
- **向量操作动机**
  ```c
  // 传统循环执行
  for (int i = 0; i < N; i++) {
      A[i] = B[i] + C[i];
  }
  ```
  ```c
  // SIMD并行执行
  for (int i = 0; i < N; i += 4) {
      // 并行执行4个元素的运算
      A[i:i+3] = B[i:i+3] + C[i:i+3];
  }
  ```
- **x86向量指令集**
  - **SSE2**
    * 4路并行浮点和整数指令
    * 支持：Intel Pentium 4及以后、AMD Athlon 64及以后
  - **AVX**
    * 8路并行浮点和整数指令
    * 支持：Intel Sandy Bridge、AMD Bulldozer及以后
- **线程级并行**
- **线程组成**
  - 指令流
  - 私有资源：PC、寄存器、栈
  - 共享资源：全局变量、堆
- **同时多线程(SMT)**
  - 优点：
    * 硬件资源复制最小化
    * 提供更多乱序执行机会
  - 缺点：
    * 缓存和执行资源竞争可能降低单线程性能
- **多核架构**
  - 复制完整的处理器核心
  - 例如：Sandy Bridge-E有6个核心
  - 优点：
    * 核心间资源独立，仅共享最后级缓存
    * 更容易利用摩尔定律
  - 缺点：
    * 资源利用率问题
- **CPU设计总结**
- **优化目标**
  - 针对顺序编程优化
    * 使用流水线、分支预测、超标量、乱序执行等技术
    * 通过高时钟速度和高利用率减少执行时间
- **主要挑战**
  - 内存访问速度慢是持续存在的问题
  - 并行度有限
    * Sandy Bridge-E适合6-12个活跃线程
    * 难以支持32K线程级并行



## GPU架构与设计

### 基础架构与工作负载
- **GPU发展历史**
- **早期阶段（90年代初）**
  - 无GPU时代，依赖软件渲染
  - 代表作品：
    * Wolfenstein 3D (1992)
    * Doom (1993)
  - 特点：
    * 交互式软件渲染
    * PC平台3D图形的开端
    * 当时超级计算机已有交互式渲染能力
- **图形工作负载特征**
- **处理对象**
  - 三角形/顶点(Triangles/vertices)
  - 像素/片段(Pixels/fragments)
- **特点**
  - 大规模并行处理
  - 数据独立性高
  - 计算密集型
- **为什么需要GPU**
- **工作负载特点**
  - 图形处理天然并行("embarrassingly parallel")
  - 两种并行性：
    * 数据并行(Data-parallel)
    * 流水线并行(Pipeline-parallel)
- **硬件支持**
  - 纹理过滤(Texture filtering)
  - 光栅化(Rasterization)
  - CPU和GPU可并行执行
- **GPU通用计算应用**
- **超越图形处理**
  - 布料模拟(Cloth simulation)
  - 粒子系统(Particle system)
  - 矩阵乘法(Matrix multiply)
- **适用特点**
  - 高度并行的计算任务
  - 数据独立性强
  - 计算密集型问题



### 执行模型与处理单元

- **Shader编程模型**
- **特点**
  - 片段独立处理
  - 无显式并行编程
  - 每个片段有独立的逻辑控制流
- **处理流程**
  1. 编译着色器
     - 输入：着色器源代码
     - 输出：指令序列
  2. 执行着色器
     - 取指/解码(Fetch/Decode)
     - ALU执行(Execute)
     - 执行上下文(Execution Context)
- **GPU的ALU设计**
- **基本架构**
  - 多ALU并行设计
  - **共享的取指/解码单元**
  - 共享的上下文数据区域
- **设计思路**
  - 分摊指令流管理成本
  - SIMD(单指令多数据)处理
  - 8个ALU为一组
  - 多组ALU并行工作
- **大规模并行处理**
- **并行规模**
  - 128个片段并行处理
  - 16个处理核心
  - 每个核心8个ALU
  - 16个并行指令流
- **处理层次**
  - 顶点(Vertices)处理
  - 图元(Primitives)处理
  - 片段(Fragments)处理
- **数据流**
  - OpenGL/DirectX工作项
  - 128个并行工作单元
  - 共享指令流
- **分支处理**
- **问题**
  - 条件分支导致SIMD效率降低
  - 不同分支路径导致部分ALU空闲
  - 示例代码：
    ```c
    if (x > 0) {
        y = pow(x, exp);
        y *= k5;
        refl = y + k8;
    } else {
        x = 0;
        refl = k8;
    }
    ```
- **性能影响**
  - 最坏情况：性能降至1/8
  - 原因：分支导致部分ALU无法执行有效工作
  - 解决方案：尽量避免复杂的分支逻辑



### 性能优化与内存管理

- **停顿问题**
- **定义**
  - 核心由于依赖关系无法执行下一条指令
  - 主要来源：纹理访问延迟(100-1000个周期)
- **缓存作用**
  - 减少内存访问延迟
  - 为处理器提供高带宽数据访问
  - L3缓存(8MB)作为内存访问缓冲
- **延迟隐藏技术**
- **基本思路**
  - 在单个核心上交错处理多个片段
  - 利用大量独立片段掩盖高延迟操作
- **实现方式**
  - 当一组片段停顿时切换到其他组
  - 通过并行执行掩盖延迟
  - 增加单个组的运行时间以提高多组吞吐量
- **上下文存储设计**
- **存储池**
  - 总容量：128 KB
  - 灵活分配给不同大小的上下文
- **三种配置方案**
  1. **18个小上下文**
     - 最大化延迟隐藏能力
     - 适合高延迟操作场景
  2. **12个中等上下文**
     - 平衡延迟隐藏和资源使用
  3. **4个大上下文**
     - 低延迟隐藏能力
     - 适合低延迟操作场景
- **GPU vs CPU内存层次**
- **CPU特点**
  - 大缓存，低延迟
  - 针对少量线程优化
  - 复杂的分支预测和预取
- **GPU特点**
  - 高带宽，高延迟
  - 通过大量线程隐藏延迟
  - 简化的缓存结构
- **GPU架构示例**
- **典型配置**
  - 16个核心
  - 每核心8个ALU（总计128个）
  - 16个并行指令流
  - 8个并发（但交错）着色器流
  - 512个并发片段
  - 时钟频率：1GHz



## AI时代的GPU架构

### 内存与计算优化
- **AI与内存墙挑战**
- **模型规模增长**
  - Transformer模型规模快速增长
  - 2016-2022年间显著扩大
  - 内存需求呈指数级增长
- **内存墙问题**
  - 计算能力增长快于内存带宽
  - 内存访问成为性能瓶颈
  - 需要新的架构设计
- **计算内存一体化**
- **深度学习需求**
  - 需要存储整个模型用于训练和推理
  - 大量参数和中间结果
- **设计挑战**
  - 物理限制导致无法简单增加每芯片内存
  - 需要优化内存访问模式
- **解决方案**
  - 将内存放置在靠近计算单元的位置
  - 优化数据传输路径
  - 提高内存访问效率
- **NVIDIA H100 GPU**
- **硬件规格**
  - 80GB VRAM
  - 80个流式多处理器(SM)
  - 18,432个CUDA核心
- **计算性能**
  - 67 TFLOPS (FP32)
  - 4 PFLOPS (FP8)
- **架构特点**
  - 高带宽内存
  - 大规模并行处理能力
  - 针对AI工作负载优化

### 多GPU系统与扩展
- **GPU集群架构**
- **处理器集群**
  - 物理相邻的SM组
  - 共享资源和数据
  - 优化通信效率
- **分布式共享内存**
  - SM间直接网络连接
  - 避免全局内存访问
  - 降低数据传输开销
- **多GPU系统**
- **NVIDIA DGX H100**
  - 8个H100 GPU
  - 640GB总内存
  - 32 PFLOP (FP8)性能
- **GPU互联技术**
  - NVLink: 900GB/s GPU间带宽
  - NVSwitch: 7.2TB/s总带宽
  - 比PCIe Gen5快7倍
- **数据中心扩展**
- **DGX POD**
  - 多DGX系统组合
  - InfiniBand网络互联
    * 400 GB/s数据传输速率
- **DGX SuperPOD**
  - 32+个DGX系统
  - 1 exaFLOP (FP8)算力
  - 大规模AI训练能力
- **CUDA自动扩展**
- **编程模型优势**
  - 自动管理并行度
  - 简化多GPU编程
  - 高效利用硬件资源



## 并行算法设计

### 基础并行算法
- **并行规约**
- **定义**
  - 从一组数据计算出单个结果的操作
  - 并行方式完成规约计算
- **应用场景**
  - 求和(Sum)
  - 最大值(Maximum)
  - 最小值(Minimum)
  - 乘积(Product)
  - 平均值(Average)
- **并行规约实现**
- **基本特点**
  - 类似篮球锦标赛的淘汰赛制
  - 需要log(n)轮迭代
- **算法步骤**
  ```
  for d = 0 to log₂n - 1
      for all k = 0 to n-1 by 2^(d+1) in parallel
          x[k] += x[k + 2^d]
  ```
- **性能考虑**
  - 算术强度：计算与内存访问比率
  - 原地修改数组以提高效率
- **扫描算法**
- **前缀和(All-Prefix-Sums)**
  - **输入**
    * n个元素的数组：[a₀, a₁, a₂, ..., aₙ₋₁]
    * 二元关联运算符：⊕
    * 单位元：I
  - **输出**：[I, a₀, (a₀⊕a₁), ..., (a₀⊕a₁⊕...⊕aₙ₋₂)]
- **扫描类型**
- **独占扫描(Exclusive Scan)**
  - 结果不包含当前元素
  - 示例：
    * 输入：[3 1 7 0 4 1 6 3]
    * 输出：[0 3 4 11 11 15 16 22]
- **包含扫描(Inclusive Scan)**
  - 结果包含当前元素
  - 示例：
    * 输入：[3 1 7 0 4 1 6 3]
    * 输出：[3 4 11 11 15 16 22 25]



### 高效扫描实现

- **朴素并行扫描**
  ```c
     for d = 1 to log_n
       for all k in parallel
           if (k >= 2^(d-1))
               x[k] = x[k - 2^(d-1)] + x[k]
  ```
- **单线程实现**
  ```c
  out[0] = in[0];  // 假设n > 0
  for (int k = 1; k < n; ++k)
      out[k] = out[k-1] + in[k];
  ```
- **并行实现特点**
  - 需要log(n)轮迭代
  - 每轮迭代中并行处理不同位置
  - 每个线程读取两个值并写入一个结果

- **具体示例**
  - 输入数组：[3 1 7 0 4 1 6 3]
  - 目标：计算前缀和
  - 迭代过程：
    ```
    初始数组：    [3  1  7  0  4  1  6  3]
    
    第1轮迭代(d=1)：每个元素和前一个元素相加
    [3  4  8  7  4  5  7  9]
     ↑  ↑  ↑  ↑  ↑  ↑  ↑  ↑
     3  3+1 7+1 0+7 4+0 1+4 6+1 3+6
    
    第2轮迭代(d=2)：每个元素和前两个位置的元素相加
    [3  4  11 11 12 12 12 16]
     ↑  ↑  ↑  ↑  ↑  ↑  ↑  ↑
     3  4  8+3 7+4 4+8 5+7 7+5 9+7
    
    第3轮迭代(d=3)：每个元素和前四个位置的元素相加
    [3  4  11 11 15 16 22 25]
    ```
  - **并行优势**
    * 每轮迭代中的所有计算可以同时进行
    * 不同线程间无数据竞争
    * 虽需要log₂(n)轮迭代，但每轮并行度高

- **工作效率优化的并行扫描**
- **平衡二叉树结构**
  - n个叶子节点 = log₂n层
  - 每层d有2^d个节点
  - 数组表示：
    * 左子节点索引 = n/2（向下取整）
    * 右子节点索引 = n

- **算法两个阶段**
  1. **Up-Sweep (并行规约)**
     - 自底向上构建树
     - 每个节点存储子树的和
     - O(n)次加法操作
  
  2. **Down-Sweep (构建扫描)**
     - 自顶向下传播部分和
     - 设置根节点为0
     - 每个节点：
       * 将当前值传给左子节点
       * 右子节点值 = 左子节点旧值 + 当前值
     - O(n)次加法和交换操作

- **性能特点**
  - 总计O(n)次操作
  - 可在独占扫描和包含扫描间转换
  - 比朴素方法更高效

- **具体示例**
  - 输入数组：[3 1 7 0 4 1 6 3]
  - 目标：计算前缀和
  - **Up-Sweep阶段**（自底向上构建树）：
    ```
    第0层（叶子节点）：    3  1  7  0  4  1  6  3   原始数组
                          ↙  ↘ ↙  ↘ ↙ ↘ ↙ ↘
    第1层：               4      7     5    9      相邻两个数相加
                           ↘ ↙  ↘  ↙ ↘
    第2层：                   11    14         两个和相加
                             ↘  ↙
    第3层（根节点）：             25           最终总和
    ```

  - **Down-Sweep阶段**（自顶向下构建扫描）：
    ```
                             25  
                           ↙  ↘
                         11     14  
                        ↙  ↘   ↙  ↘      
                      4     7   5    9    
                    ↙ ↘  ↙ ↘  ↙ ↘  ↙ ↘
                   3   1  7   0  4  1  6   3   
    
    
    第3层（根）：              0            设置根节点为0
                           ↙  ↘
    第2层：                0       11         左子节点继承父节点值
                        ↙  ↘   ↙  ↘      右子节点 = 左子节点旧值 + 父节点值
    第1层：            0     4   11    14    
                    ↙ ↘  ↙ ↘  ↙ ↘  ↙ ↘
    最终结果：        0  3  4  11 11 15 16 22   这就是前缀和
    ```
    
  - **过程说明**
    
    1. Up-Sweep阶段：
       * 将数组视为树的叶子节点
       * 每层并行计算相邻节点的和
       * 最终得到数组总和
    2. Down-Sweep阶段：
       * 根节点设为0
       * 每个节点并行处理：
         - 左子节点继承父节点值
         - 右子节点 = 左子节点旧值 + 父节点值
       * 最后得到前缀和数组
    
  - **算法优势**
    * 每层操作都可以并行执行
    * 总计只需O(n)次操作
    * 避免了重复计算
    * 数据访问模式更规律

- **扫描算法扩展**
  - **扫描算法的限制**
    - 要求数组长度为2的幂
    - 在单个块内执行（除非使用全局内存）
    - 块大小限制：最多为线程数的两倍

  - **处理任意长度数组的方法**
    1. **分块处理**
       ```
       原始数组：[2 5 1 7 4 2 8 3 2 8 1 5]
       分成块：   [2 5 1 7] [4 2 8 3] [2 8 1 5]
       ```

    2. **并行扫描每个块**
       ```
       块1扫描结果：[0 2 7 8]  累积和=15
       块2扫描结果：[0 4 6 14] 累积和=17
       块3扫描结果：[0 2 10 11] 累积和=16
       ```

    3. **收集块的总和**
       ```
       块总和数组：[15 17 16]
       ```

    4. **对块总和进行扫描**
       ```
       块增量：[0 15 32]
       ```

    5. **将块增量加回对应块**
       ```
       最终结果：
       块1：[0 2 7 8]
       块2：[15 19 21 29]
       块3：[32 34 42 43]
       ```

  - **非2的幂长度处理**
    - 对最后一个块进行零填充
    - 示例：
      ```
      原始长度：14（非2的幂）
      填充后：16（补充两个0）
      ```

  - **性能优化**
    1. **块大小选择**
       - 考虑SM的线程数限制
       - 平衡并行度和资源使用
    
    2. **内存访问优化**
       - 使用共享内存存储块内数据
       - 合并全局内存访问
    
    3. **同步开销**
       - 最小化块间同步点
       - 利用块内线程同步

  - **实现注意事项**
    1. **边界处理**
       - 正确处理最后一个不完整块
       - 避免越界访问
    
    2. **工作分配**
       - 均匀分配工作负载
       - 避免线程分支分化
    
    3. **内存管理**
       - 有效利用共享内存
       - 减少全局内存访问



### 应用算法

- **流压缩**
- **定义与目标**
  - 给定一个元素数组
  - 创建一个新数组，仅包含满足特定条件的元素（如非零元素）
  - 保持元素的相对顺序不变

- **应用场景**
  - 路径追踪中的碰撞检测
  - 稀疏矩阵压缩
  - 可以减少从GPU到CPU传输的数据量

- **算法步骤**
  1. **标记阶段（并行执行）**
     - 为每个元素创建临时数组
     - 满足条件的元素标记为1
     - 不满足条件的元素标记为0
     - 示例：
       ```
       输入数组：    [■ □ ■ ■ □ ■ □ ■]
       临时数组：    [1 0 1 1 0 1 0 1]  // ■=1, □=0

       输入数组：    [□ ■ ■ ■ □ ■ □ ■]
       临时数组：    [0 1 1 1 0 1 0 1]  // ■=1, □=0
       ```

  2. **扫描阶段（并行执行）**
     - 对临时数组执行独占扫描(Exclusive Scan)
     - 扫描结果作为目标数组的索引
     - 示例：
       ```
       临时数组：    [1 0 1 1 0 1 0 1]
       扫描结果：    [0 1 1 2 3 3 4 4]  // exclusive scan

       临时数组：    [0 1 1 1 0 1 0 1]
       扫描结果：    [0 0 1 2 3 3 4 4]  // exclusive scan
       ```

  3. **分散阶段（并行执行）**
     - 仅当临时数组中对应位置为1时写入元素
     - 使用扫描结果作为写入位置
     - 示例：
       ```
       原始数组：    [■ □ ■ ■ □ ■ □ ■]
       临时数组：    [1 0 1 1 0 1 0 1]
       扫描结果：    [0 1 1 2 3 3 4 4] // 写入位置
       最终数组：    [■ ■ ■ ■ ■]  // 仅保留非空元素
       ```

- **性能特点**
  - 三个阶段都可以并行执行
  - 总体复杂度为O(n)
  - 高效利用GPU并行计算能力
  - 每个阶段都避免了线程间的数据竞争

- **实现注意事项**
  1. **标记阶段**
     - 每个线程独立判断一个元素
     - 无需同步，完全并行
  
  2. **扫描阶段**
     - 使用高效的并行扫描算法
     - 可以选择朴素或工作效率优化的实现
  
  3. **分散阶段**
     - 使用扫描结果作为写入位置
     - 避免写入冲突
     - 保持原始元素顺序

- **求和区域表**
- **定义**
  - 2D表格，每个元素存储从左上角到当前位置的所有元素之和
  - 用于快速计算图像中任意矩形区域的和

- **基本原理**
  - 输入：2D图像或数据
  - 输出：每个位置(x,y)存储(0,0)到(x,y)矩形区域的和
  - 示例：
    ```
    输入图像：        SAT结果：
    [1 2 3 4]        [1  3  6  10]
    [2 2 3 4]   →    [3  7  13 21]
    [3 3 3 4]        [6  13 22 34]
    [4 4 4 4]        [10 21 34 50]
    ```

- **应用场景**
  - 图像处理中的可变宽度滤波
  - 用于近似景深效果
  - 环境光遮蔽计算
  - 光照和反射计算

- **计算公式**
  ```
  // 计算任意矩形区域的和
  f_avg = (SAT[xr,yb] - SAT[xr,yt-1] - SAT[xl-1,yb] + SAT[xl-1,yt-1]) / (w × h)
  其中：
  - (xl,yt)是左上角坐标
  - (xr,yb)是右下角坐标
  - w和h是矩形的宽度和高度
  ```

- **GPU实现方法**
  1. **行扫描**
     - 对每一行执行包含扫描
     - 所有行可以并行处理
     - 示例：
       ```
       原始行：    [1 2 3 4]
       扫描结果：  [1 3 6 10]
       ```

  2. **列扫描**
     - 对每一列执行包含扫描
     - 所有列可以并行处理
     - 示例：
       ```
       原始列：    [1]    扫描结果：  [1]
                  [2]                [3]
                  [3]      →        [6]
                  [4]                [10]
       ```

- **性能优化**
  - 利用GPU并行计算能力
  - 行列扫描可以完全并行
  - 避免重复计算
  - 使用纹理缓存优化内存访问

- **实现注意事项**
  1. **边界处理**
     - 处理图像边缘情况
     - 适当的填充策略
  
  2. **数值精度**
     - 考虑累加过程中的精度损失
     - 选择合适的数据类型
  
  3. **内存访问模式**
     - 优化数据布局减少缓存缺失
     - 利用GPU纹理缓存特性

- **基数排序**
- **基本特点**
  - 适用于小键值的高效排序
  - k位键值需要k次遍历
  - 每次遍历基于一个比特位进行分区

- **排序过程**
  1. **从最低有效位(LSB)开始**
     - 按位进行排序
     - 逐步向最高有效位(MSB)移动
     - 保持相对顺序（稳定排序）

- **并行基数排序步骤**
  1. **分块(Tiling)**
     - 将输入数组分成多个块
     - 每个块大小适合SM的共享内存
  
  2. **并行排序**
     - 对每个块并行执行基数排序
     - 使用split函数进行位分区
  
  3. **并行归并**
     - 使用并行双调归并
     - 直到所有块都被合并

- **Split函数实现**
  给定：
  ```
  - 数组a：要排序的键值
  - 数组b：表示每个位是否为true/false
  - n：当前处理的位
  ```
  
  步骤：
  1. **计算e数组**
     - e[i] = !b[i]（取反值）
  
  2. **扫描e数组**
     - f = exclusive_scan(e)
  
  3. **计算totalFalses**
     - totalFalses = sum(e)
  
  4. **计算t数组**
     - t[i] = b[i] ? f[i] + totalFalses : f[i]
  
  5. **分散操作**
     - 根据t数组中的地址重排数据
     - false值放在前面，true值放在后面

- **具体示例**
  ```
  输入数组：    [202 511 200 510 211 121 201 202]
  处理位n=1：   [0   1   0   0   1   1   1   0]  // b数组
  
  1. 计算e：    [1   0   1   1   0   0   0   1]  // e = !b
  2. 扫描e：    [0   1   1   2   3   3   3   3]  // f = scan(e)
  3. 计算total：totalFalses = 4
  4. 计算t：    [0   5   1   2   6   7   8   3]  // t数组
  5. 分散：     [202 200 510 202 511 211 121 201] // 结果
  ```

- **性能优化**
  1. **内存访问优化**
     - 使用共享内存存储块内数据
     - 合并内存访问减少事务数
  
  2. **并行度优化**
     - 选择合适的块大小
     - 平衡并行度和资源使用
  
  3. **分支优化**
     - 减少分支判断
     - 使用位运算代替条件语句

- **实现注意事项**
  1. **块大小选择**
     - 考虑共享内存大小
     - 考虑SM的并行能力
  
  2. **归并策略**
     - 选择高效的并行归并算法
     - 避免过多的同步点
  
  3. **位处理优化**
     - 使用位运算加速
     - 考虑不同数据类型的特点



## GPU性能优化

### 最大化利用率
- **并行规约优化**
  - 重新审视并行规约算法
  - 优化实现以提高性能
  - **优化示例**
    1. **基本实现**
       ```c
       __shared__ float partialSum[];
       // ... load into shared memory
       unsigned int t = threadIdx.x;
       // 1, 2, 4, 8
       for (int stride = 1; stride < blockDim.x; stride *= 2) {
           __syncthreads();
           if (t % (2 * stride) == 0)
               partialSum[t] += partialSum[t + stride];
       }
       ```
       
    2. **优化实现**
       ```c
       // Mirror Optimized
       __shared__ float partialSum[];
       // ... load into shared memory
       unsigned int t = threadIdx.x;
       // 8 -> 4 -> 2 -> 1
       for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
           __syncthreads();
           if (t < stride)
               partialSum[t] += partialSum[t + stride];
       }
       ```
       
    3. **主要优化点**
       
       - 减少了线程分支分化
       - 优化了内存访问模式
       - 减少了活跃线程数量
       - 每一轮迭代的线程数减半
       
    4. **性能差异**
       - 基本实现：stride = 1, 2, 4, ...
       - 优化实现：stride = n/2, n/4, n/8, ...
       - 优化版本避免了不必要的条件检查
       - 减少了线程间的等待时间
  
  5. **Mirror索引优化示例**
     ```
     原始数组：[3 1 7 0 4 1 6 3]
     
     第1轮迭代：stride = 1
     线程0：3 + 1 = 4
     线程2：7 + 0 = 7
     线程4：4 + 1 = 5
     线程6：6 + 3 = 9
     结果：[4 1 7 0 5 1 9 3]
     
     第2轮迭代：stride = 2
     线程0：4 + 7 = 11
     线程4：5 + 9 = 14
     结果：[11 1 7 0 14 1 9 3]
     
     第3轮迭代：stride = 4
     线程0：11 + 14 = 25
     结果：[25 1 7 0 14 1 9 3]
     ```
     
  6. **性能对比分析**
     - **原始实现**
       * 每轮迭代：所有线程都要检查条件
       * 线程利用率：随迭代递减
       * 示例：8个线程 → 4个 → 2个 → 1个
     
     - **优化实现**
       * 每轮迭代：只有需要的线程参与
       * 线程利用率：更高
       * 示例：4个线程 → 2个 → 1个
     
     - **Mirror索引实现**
       * 内存访问更连续
       * 减少了分支判断
       * 线程利用率最优
  
  7. **不同步长的效果比较**
     ```
     步长递增 (stride *= 2)：
     步长序列：1, 2, 4, 8, ...
     活跃线程：n/2, n/4, n/8, ...
     条件判断：t % (2*stride) == 0
     
     步长递减 (stride >>= 1)：
     步长序列：n/2, n/4, n/8, ...
     活跃线程：n/2, n/4, n/8, ...
     条件判断：t < stride
     
      - **基本实现中的线程行为**
     ```
        stride = 1 时：
        线程0：检查 0%2=0，执行
        线程1：检查 1%2=1，不执行
        线程2：检查 2%2=0，执行
        线程3：检查 3%2=1，不执行
        线程4：检查 4%2=0，执行
        线程5：检查 5%2=1，不执行
        线程6：检查 6%2=0，执行
        线程7：检查 7%2=1，不执行
     
        stride = 2 时：
        线程0：检查 0%4=0，执行
        线程1：检查 1%4=1，不执行
        线程2：检查 2%4=2，不执行
        线程3：检查 3%4=3，不执行
        线程4：检查 4%4=0，执行
        线程5：检查 5%4=1，不执行
        线程6：检查 6%4=2，不执行
        线程7：检查 7%4=3，不执行
        ```
     
      - **优化实现中的线程行为**
        ```
        stride = 4 时：
        线程0：检查 0<4，执行
        线程1：检查 1<4，执行
        线程2：检查 2<4，执行
        线程3：检查 3<4，执行
        线程4：检查 4<4, 不执行
        线程5：检查 5<4, 不执行
        线程6：检查 6<4, 不执行
        线程7：检查 7<4, 不执行
     
        stride = 2 时：
        线程0：检查 0<2，执行
        线程1：检查 1<2，执行
        线程2：检查 2<2，不执行
        线程3：检查 3<2，不执行
        ...其余线程已在上一轮被取消
        ```
        ```
     
  8. **实际性能数据**
     - **原始实现**：
       * 第1轮：8个线程，4次加法
       * 第2轮：8个线程，2次加法
       * 第3轮：8个线程，1次加法
       * 总计：24个线程检查，7次加法
     
     - **优化实现**：
       * 第1轮：4个线程，4次加法
       * 第2轮：2个线程，2次加法
       * 第3轮：1个线程，1次加法
       * 总计：7个线程检查，7次加法
  
- **Warp分区**
  - **基本概念**
    - 每个warp固定32个线程
    - 最后一个warp自动填充
  
  - **分区规则**
    1. **1D块分区**
       - threadIdx.x的范围：0 ~ 1023（Fermi及更新架构）
       - Warp n从线程32n开始
       - 结束于线程32(n + 1) - 1
       - 如果块大小不是32的倍数，最后一个warp会被填充
    
    2. **2D块分区**
       - 基于递增的threadIdx
       - 从行threadIdx.y=0开始
       - 每行内部threadIdx.x连续增加
    
    3. **3D块分区**
       - 从threadIdx.z=0开始
       - 按2D块方式处理每个z层
       - 增加threadIdx.z并重复
  
  - **分支分化问题**
    - **关键问题**：分支分化发生在warp内部
    - **性能影响**
      * 如果warp内的线程执行不同分支
      * 不同分支会被序列化执行
      * 可能导致性能显著下降
    
    - **分支分化示例**
      ```c
      // 可能导致分支分化的代码
      if (threadIdx.x > 15) {
          // 分支1
      } else {
          // 分支2
      }
      ```
  
  - **优化策略**
    1. **最小化warp内分支分化**
       - 重构算法使同一warp内的线程执行相同路径
       - 在warp边界对齐分支条件
    
    2. **提前退出warp**
       - 识别并退出不再需要的warp
       - 节省SM时钟时间
       - 使更多warp可被调度
    
    3. **并行规约优化示例**
       ```c
       // 更好的实现（减少分支分化）
       if (t < stride) {
           partialSum[t] += partialSum[t + stride];
       }
       // 而不是
       if (t % (2 * stride) == 0) {
           partialSum[t] += partialSum[t + stride];
       }
       ```
  
  - **性能考虑**
    - 构造/重构算法以最小化warp内分支分化
    - 最好情况：无分支分化
    - 最坏情况：有限的分支分化
    - 尽早退出warp以节省SM时钟时间
    - 使更多warp可用于调度



### 最大化内存吞吐量

#### Bank冲突(Bank Conflicts)

1. **共享内存结构**
   - 内存被分为多个bank（通常是32个）
   - 每个bank每个时钟周期可以服务一个地址
   - 连续的32位字被分配到连续的bank

2. **冲突类型**
   - **无冲突情况**
     * 所有线程访问不同bank
     * 多个线程访问同一bank的同一地址（广播）
   - **N路冲突情况**
     * N个线程访问同一bank的不同地址
     * 访问被序列化，延迟增加N倍

3. **性能影响**
   - 无冲突：一个时钟周期完成
   - N路冲突：N个时钟周期完成
   - 填充可能浪费一些内存但能提高性能

4. **避免策略**
   - **填充技术**
     * 在数组维度上添加一个元素作为填充
     * 改变bank映射模式
   - **访问模式优化**
     * 使用连续的访问模式
     * 避免跨步访问
   - **广播访问**
     * 多个线程读取同一地址时使用广播
     * 避免产生bank冲突

5. **优化策略**
   ```c
   // 不推荐（2路冲突）
   shared[threadIdx.x * 2]
   
   // 推荐（无冲突）
   shared[threadIdx.x]
   ```

#### 广播机制(Broadcast)

1. **基本原理**
   - 当同一warp中的多个线程访问同一bank中的同一地址时
   - 数据会被同时广播给所有请求的线程
   - 这种访问模式不会产生bank冲突

2. **硬件实现**
   - bank只需读取一次数据
   - 将读取的数据同时发送给所有请求的线程
   - 不需要序列化访问

3. **常见使用场景**
   ```c
   // 共享读取
   float value = shared[0];
   
   // 行数据读取
   float row_value = shared[row * width];
   
   // 规约操作
   if (threadIdx.x < blockDim.x/2) {
       shared[threadIdx.x] += shared[threadIdx.x + blockDim.x/2];
   }
   ```

### 最大化指令吞吐量

#### 数据预取(Data Prefetching)

1. **基本原理**
   - 在数据实际被使用前提前从全局内存加载
   - 利用独立指令掩盖内存访问延迟
   - 增加全局内存读取和使用之间的指令数量

2. **代码示例**
   ```c
   float m = M[i];          // 开始内存读取
   float f = a * b + c * d; // 执行独立计算
   float f2 = m * f;        // 使用已加载的数据
   ```

3. **矩阵乘法中的预取优化**
   ```c
   // 使用预取的分块矩阵乘法
   // 加载第一个块到寄存器
   for (/* ... */) {
       // 将寄存器数据存入共享内存
       __syncthreads();
       // 加载下一个块到寄存器
       // 计算点积
       __syncthreads();
   }
   ```

#### 循环展开
```c
// 原始循环
for (int k = 0; k < BLOCK_SIZE; k++) {
    PValue += M[ty][k] * N[k][tx];
}

// 展开后
PValue += M[ty][0] * N[0][tx];
PValue += M[ty][1] * N[1][tx];
// ... BLOCK_SIZE = 16
PValue += M[ty][15] * N[15][tx];

// 自动展开
#pragma unroll BLOCK_SIZE
for (int k = 0; k < BLOCK_SIZE; k++) {
    PValue += M[ty][k] * N[k][tx];
}
```

#### 线程粒度优化
```c
// 细粒度
float result = input[threadIdx.x];

// 粗粒度
float result1 = input[threadIdx.x * 2];
float result2 = input[threadIdx.x * 2 + 1];
```

**选择考虑**：
- 细粒度：高并行度，低资源使用
- 粗粒度：好数据重用，高资源使用
- 根据问题特点和硬件限制选择



## NVIDIA NSight Graphics工具

### 工具概述

1. **基本功能**
   - 图形应用程序调试和分析
   - 性能优化和问题诊断
   - 实时渲染分析
   - API调用跟踪

2. **主要组件**
   - **Frame Debugger**：逐帧分析渲染过程
   - **GPU Trace**：分析GPU性能和吞吐量
   - **Shader Debugger**：调试着色器代码
   - **API Inspector**：检查API调用和状态

### Frame Debugger功能

1. **基本操作**
   - **控制选项**
     * Disconnect：停止调试但程序继续运行
     * Next Frame：分析下一帧
     * Resume：继续执行

2. **Event Viewer**
   - 显示帧内命令执行顺序
   - 提供API调用详细信息
   - 支持事件过滤和搜索

3. **资源分析**
   - 检查API调用参数
   - 分析渲染状态
   - 查看资源绑定
   - 分析纹理和缓冲区

### 性能分析功能

1. **API Statistics**
   - 统计API调用频率
   - 分析调用耗时
   - 识别性能瓶颈

2. **性能监控**
   - 帧时间分析
   - GPU利用率监控
   - 内存使用跟踪
   - 带宽分析

### 使用最佳实践

1. **调试技巧**
   - 使用事件过滤缩小范围
   - 合理设置捕获点
   - 结合性能计数器分析

2. **性能优化**
   - 关注高耗时API调用
   - 优化资源访问模式
   - 减少状态切换


